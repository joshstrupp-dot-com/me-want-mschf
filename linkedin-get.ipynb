{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This script is a LinkedIn employee scraper specifically designed to collect information about MSCHF employees.\n",
    "- It uses Selenium WebDriver to automate browser interactions and navigate through LinkedIn search results.\n",
    "\n",
    "# The script includes functionality for:\n",
    "- Browser initialization with Chrome\n",
    "- LinkedIn authentication\n",
    "- Scraping employee data across multiple pages\n",
    "- Data storage in a structured format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium_mschf_employee_scraper.py\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "USERNAME = \"joshstrupp@gmail.com\"      # ← LinkedIn email\n",
    "PASSWORD = \"Stonerjoe1\"          # ← LinkedIn password\n",
    "SEARCH_URL = (\n",
    "    \"https://www.linkedin.com/search/results/people/\"\n",
    "    \"?currentCompany=%5B%2218430865%22%5D\"\n",
    "    \"&heroEntityKey=urn%3Ali%3Aorganization%3A18430865\"\n",
    "    \"&keywords=mschf&origin=FACETED_SEARCH\"\n",
    "    \"&position=0&searchId=0c69a3cb-25e8-45a2-adf1-3e6c0b681c6c&sid=fC%40\"\n",
    ")\n",
    "NUM_PAGES = 15\n",
    "IMPLICIT_WAIT = 5   # seconds\n",
    "\n",
    "\n",
    "def init_driver() -> webdriver.Chrome:\n",
    "    \"\"\"Initialize Chrome WebDriver with basic options.\"\"\"\n",
    "    chrome_opts = Options()\n",
    "    chrome_opts.add_argument(\"--start-maximized\")\n",
    "    # chrome_opts.add_argument(\"--headless\")  # ← optional, but headless is more detectable\n",
    "    service = Service()  # Assumes chromedriver is in PATH\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_opts)\n",
    "    driver.implicitly_wait(IMPLICIT_WAIT)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def linkedin_login(driver: webdriver.Chrome):\n",
    "    \"\"\"Log into LinkedIn with USERNAME / PASSWORD.\"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://www.linkedin.com/login\")\n",
    "        time.sleep(3)  # Increased wait time for page load\n",
    "\n",
    "        # 1. Enter credentials with human-like delays\n",
    "        username_field = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"username\"))\n",
    "        )\n",
    "        password_field = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"password\"))\n",
    "        )\n",
    "        \n",
    "        # Type username with random delays\n",
    "        for char in USERNAME:\n",
    "            username_field.send_keys(char)\n",
    "            time.sleep(0.1)  # Small delay between keystrokes\n",
    "        \n",
    "        time.sleep(0.5)  # Pause before password\n",
    "        \n",
    "        # Type password with random delays\n",
    "        for char in PASSWORD:\n",
    "            password_field.send_keys(char)\n",
    "            time.sleep(0.1)  # Small delay between keystrokes\n",
    "\n",
    "        time.sleep(1)  # Pause before submitting\n",
    "\n",
    "        # 2. Submit form\n",
    "        password_field.submit()\n",
    "\n",
    "        # 3. Wait for either successful login or error message\n",
    "        try:\n",
    "            # Try multiple selectors for successful login\n",
    "            login_success_selectors = [\n",
    "                \"nav[role='navigation']\",\n",
    "                \"#global-nav-search\",\n",
    "                \"input.search-global-typeahead__input\",\n",
    "                \"div[data-test-id='nav-search-typeahead']\"\n",
    "            ]\n",
    "            \n",
    "            login_success = False\n",
    "            for selector in login_success_selectors:\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                    )\n",
    "                    login_success = True\n",
    "                    print(f\"Login successful! Found element: {selector}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not login_success:\n",
    "                # Check for various error conditions\n",
    "                error_conditions = [\n",
    "                    (By.CLASS_NAME, \"alert-content\"),\n",
    "                    (By.CLASS_NAME, \"form__error\"),\n",
    "                    (By.CLASS_NAME, \"error-for-password\"),\n",
    "                    (By.CLASS_NAME, \"error-for-username\")\n",
    "                ]\n",
    "                \n",
    "                for by, value in error_conditions:\n",
    "                    try:\n",
    "                        error_element = driver.find_element(by, value)\n",
    "                        print(f\"Login failed: {error_element.text}\")\n",
    "                        raise Exception(f\"Login failed: {error_element.text}\")\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                # If no specific error found, check for security challenges\n",
    "                if \"security\" in driver.current_url.lower():\n",
    "                    print(\"Security challenge detected!\")\n",
    "                    raise Exception(\"LinkedIn security challenge detected\")\n",
    "                \n",
    "                print(\"Login failed: Could not detect navigation bar or error message\")\n",
    "                print(\"Current URL:\", driver.current_url)\n",
    "                print(\"Page title:\", driver.title)\n",
    "                raise Exception(\"Login verification failed\")\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Login timeout - page might be loading slowly or blocked\")\n",
    "            print(\"Current URL:\", driver.current_url)\n",
    "            print(\"Page title:\", driver.title)\n",
    "            raise\n",
    "\n",
    "        time.sleep(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during login: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def scrape_page(driver: webdriver.Chrome) -> list[dict]:\n",
    "    \"\"\"\n",
    "    On the current search page, find every <li> that contains\n",
    "    a child <div data-view-name=\"search-entity-result-universal-template\">.\n",
    "    Extract profile_url, image_url, name, title, location, badge_text.\n",
    "    \"\"\"\n",
    "    page_results = []\n",
    "\n",
    "    # 1. Wait until at least one result-block is present\n",
    "    WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((\n",
    "            By.XPATH,\n",
    "            \"//li[.//div[@data-view-name='search-entity-result-universal-template']]\"\n",
    "        ))\n",
    "    )\n",
    "\n",
    "    # 2. Find all such <li> elements\n",
    "    li_elements = driver.find_elements(\n",
    "        By.XPATH,\n",
    "        \"//li[.//div[@data-view-name='search-entity-result-universal-template']]\"\n",
    "    )\n",
    "\n",
    "    for li in li_elements:\n",
    "        try:\n",
    "            # Locate the inner div with the desired data attribute using find_elements for robustness\n",
    "            result_divs = li.find_elements(\n",
    "                By.XPATH,\n",
    "                \".//div[@data-view-name='search-entity-result-universal-template']\"\n",
    "            )\n",
    "            result_div = result_divs[0] if result_divs else None\n",
    "\n",
    "            entry = {}\n",
    "\n",
    "            # Profile URL (strip any query params)\n",
    "            a_tags = result_div.find_elements(By.TAG_NAME, \"a\") if result_div else []\n",
    "            entry[\"profile_url\"] = a_tags[0].get_attribute(\"href\").split(\"?\")[0] if a_tags else \"\"\n",
    "\n",
    "            # Image URL (first <img> inside the result_div)\n",
    "            img_tags = result_div.find_elements(By.TAG_NAME, \"img\") if result_div else []\n",
    "            entry[\"image_url\"] = img_tags[0].get_attribute(\"src\") if img_tags else \"\"\n",
    "\n",
    "            # Name: <span aria-hidden=\"true\">...</span>\n",
    "            name_spans = result_div.find_elements(By.XPATH, './/span[@aria-hidden=\"true\"]') if result_div else []\n",
    "            entry[\"name\"] = name_spans[0].text.strip() if name_spans else \"\"\n",
    "\n",
    "            # Title: try to find a <div> with classes t-black and t-normal, or fallback to any with t-black or t-normal\n",
    "            title_divs = result_div.find_elements(\n",
    "                By.XPATH,\n",
    "                './/div[contains(@class, \"t-black\") and contains(@class, \"t-normal\")]'\n",
    "            ) if result_div else []\n",
    "            if not title_divs and result_div:\n",
    "                title_divs = result_div.find_elements(\n",
    "                    By.XPATH,\n",
    "                    './/div[contains(@class, \"t-black\") or contains(@class, \"t-normal\")]'\n",
    "                )\n",
    "            entry[\"title\"] = title_divs[0].text.strip() if title_divs else \"\"\n",
    "\n",
    "            # Location: try to find a <div> with classes t-14 and t-normal, then fallback to a broader selector\n",
    "            location_divs = result_div.find_elements(\n",
    "                By.XPATH,\n",
    "                './/div[contains(@class, \"t-14\") and contains(@class, \"t-normal\")]'\n",
    "            ) if result_div else []\n",
    "            if (not location_divs or len(location_divs) < 2) and result_div:\n",
    "                location_divs = result_div.find_elements(\n",
    "                    By.XPATH,\n",
    "                    './/div[contains(@class, \"t-14\") or contains(@class, \"t-normal\")]'\n",
    "                )\n",
    "            if len(location_divs) >= 2:\n",
    "                entry[\"location\"] = location_divs[1].text.strip()\n",
    "            elif len(location_divs) == 1:\n",
    "                entry[\"location\"] = location_divs[0].text.strip()\n",
    "            else:\n",
    "                entry[\"location\"] = \"\"\n",
    "\n",
    "            # Badge text (if present)\n",
    "            badge_spans = result_div.find_elements(\n",
    "                By.XPATH,\n",
    "                './/span[contains(@class, \"entity-result__badge\")]'\n",
    "            ) if result_div else []\n",
    "            entry[\"badge_text\"] = badge_spans[0].text.strip() if badge_spans else \"\"\n",
    "\n",
    "            page_results.append(entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Skipped one entry:\", e)\n",
    "            print(\"HTML for skipped entry:\", li.get_attribute('outerHTML'))\n",
    "            continue\n",
    "\n",
    "    return page_results\n",
    "\n",
    "\n",
    "def click_next(driver: webdriver.Chrome) -> bool:\n",
    "    \"\"\"\n",
    "    Click the \"Next\" pagination button.\n",
    "    Return True if click succeeded, False if it's not found or disabled.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        next_btn = driver.find_element(\n",
    "            By.XPATH,\n",
    "            \"//button[contains(@aria-label, 'Next') \"\n",
    "            \"and contains(@class, 'artdeco-pagination__button--next')]\"\n",
    "        )\n",
    "        # If the button is disabled, bail out\n",
    "        if \"disabled\" in next_btn.get_attribute(\"class\"):\n",
    "            return False\n",
    "\n",
    "        next_btn.click()\n",
    "        time.sleep(3)\n",
    "        return True\n",
    "\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    driver = init_driver()\n",
    "    all_people = []\n",
    "\n",
    "    try:\n",
    "        linkedin_login(driver)\n",
    "        \n",
    "        # Add additional verification before proceeding with search\n",
    "        try:\n",
    "            # Try to find search bar again before proceeding\n",
    "            search_bar = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"input.search-global-typeahead__input\"))\n",
    "            )\n",
    "            print(\"Search bar verified before proceeding with search\")\n",
    "        except:\n",
    "            print(\"Warning: Could not verify search bar before proceeding\")\n",
    "            print(\"Current URL:\", driver.current_url)\n",
    "            print(\"Page title:\", driver.title)\n",
    "        \n",
    "        driver.get(SEARCH_URL)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Scrape pages 1 through 15\n",
    "        total_pages = 15\n",
    "        for page_num in range(1, total_pages + 1):\n",
    "            print(f\"→ Scraping page {page_num} of {total_pages} …\")\n",
    "            page_data = scrape_page(driver)\n",
    "            all_people.extend(page_data)\n",
    "\n",
    "            # Attempt to click \"Next\"; if it fails, break early\n",
    "            if not click_next(driver):\n",
    "                print(\"⛔ No more pages or Next button disabled.\")\n",
    "                break\n",
    "\n",
    "        # Build DataFrame\n",
    "        df = pd.DataFrame(all_people)\n",
    "\n",
    "        # Save to CSV (optional)\n",
    "        df.to_csv(\"mschf_linkedin_employees.csv\", index=False)\n",
    "        print(\"✅ Finished. Saved to 'mschf_linkedin_employees.csv'.\")\n",
    "        print(df.head())\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('mschf_linkedin_employees.csv')\n",
    "\n",
    "# Display first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting LinkedIn profile scraper...\n",
      "Initializing Chrome driver...\n",
      "✅ Driver initialized successfully\n",
      "\n",
      "🔑 Logging into LinkedIn...\n",
      "✅ Successfully logged into LinkedIn\n",
      "\n",
      "📖 Reading input CSV: mschf_linkedin_employees.csv\n",
      "✅ Found 150 total profiles\n",
      "📊 0 already completed, 76 remaining to process\n",
      "\n",
      "🔄 Processing profile 1/76 (Row 1): https://www.linkedin.com/in/jc-li-973721362\n",
      "\n",
      "📄 Loading profile: https://www.linkedin.com/in/jc-li-973721362\n",
      "  → Scraping About section...\n"
     ]
    }
   ],
   "source": [
    "# hi\n",
    "# Updated profile_detail_scraper.py with improved error handling and CSS selector fallbacks\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import json  # Added import for json\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────────\n",
    "INPUT_CSV    = \"mschf_linkedin_employees.csv\"\n",
    "OUTPUT_CSV   = \"mschf_profiles_detailed_full.csv\"  # Full results\n",
    "PROGRESS_PICKLE = \"scraping_progress.pkl\"  # Temporary progress file\n",
    "IMPLICIT_WAIT = 5\n",
    "PAGE_LOAD_WAIT = 3\n",
    "USERNAME = \"joshstrupp@gmail.com\"      # ← LinkedIn email\n",
    "PASSWORD = \"Stonerjoe1\"          # ← LinkedIn password\n",
    "\n",
    "# ─── SETUP ──────────────────────────────────────────────────────\n",
    "def init_driver():\n",
    "    print(\"Initializing Chrome driver...\")\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    # opts.add_argument(\"--headless\")  # optional\n",
    "    driver = webdriver.Chrome(service=Service(), options=opts)\n",
    "    driver.implicitly_wait(IMPLICIT_WAIT)\n",
    "    print(\"✅ Driver initialized successfully\")\n",
    "    return driver\n",
    "\n",
    "def linkedin_login(driver):\n",
    "    print(\"\\n🔑 Logging into LinkedIn...\")\n",
    "    driver.get(\"https://www.linkedin.com/login\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Enter credentials\n",
    "    username = driver.find_element(By.ID, \"username\")\n",
    "    password = driver.find_element(By.ID, \"password\")\n",
    "    \n",
    "    # Type username with random delays\n",
    "    for char in USERNAME:\n",
    "        username.send_keys(char)\n",
    "        time.sleep(0.1)  # Small delay between keystrokes\n",
    "    \n",
    "    time.sleep(0.5)  # Pause before password\n",
    "    \n",
    "    # Type password with random delays\n",
    "    for char in PASSWORD:\n",
    "        password.send_keys(char)\n",
    "        time.sleep(0.1)  # Small delay between keystrokes\n",
    "\n",
    "    time.sleep(1)  # Pause before submitting\n",
    "\n",
    "    # Click login button\n",
    "    driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\").click()\n",
    "    \n",
    "    # Wait for login to complete - just check if we're redirected away from login page\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            lambda driver: \"login\" not in driver.current_url.lower()\n",
    "        )\n",
    "        print(\"✅ Successfully logged into LinkedIn\")\n",
    "        return True\n",
    "    except:\n",
    "        print(\"❌ Failed to log into LinkedIn\")\n",
    "        return False\n",
    "\n",
    "# Helper function to try multiple selectors\n",
    "def find_element_with_fallbacks(parent, selectors):\n",
    "    \"\"\"Try multiple CSS selectors until one works\"\"\"\n",
    "    for selector in selectors:\n",
    "        try:\n",
    "            return parent.find_element(By.CSS_SELECTOR, selector)\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def find_elements_with_fallbacks(parent, selectors):\n",
    "    \"\"\"Try multiple CSS selectors until one works\"\"\"\n",
    "    for selector in selectors:\n",
    "        try:\n",
    "            elements = parent.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if elements:\n",
    "                return elements\n",
    "        except:\n",
    "            continue\n",
    "    return []\n",
    "\n",
    "# ─── SCRAPE ONE PROFILE ─────────────────────────────────────────\n",
    "def scrape_profile(driver, url):\n",
    "    print(f\"\\n📄 Loading profile: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(PAGE_LOAD_WAIT)\n",
    "    \n",
    "    # Check if we hit a login wall\n",
    "    if \"login\" in driver.current_url.lower():\n",
    "        print(\"⚠️ Hit login wall, attempting to log in...\")\n",
    "        if not linkedin_login(driver):\n",
    "            raise Exception(\"Failed to log in when hitting login wall\")\n",
    "        # Retry loading profile\n",
    "        driver.get(url)\n",
    "        time.sleep(PAGE_LOAD_WAIT)\n",
    "\n",
    "    data = {\"profile_url\": url}\n",
    "\n",
    "    # — About —\n",
    "    try:\n",
    "        print(\"  → Scraping About section...\")\n",
    "        # Multiple selectors to try for about section\n",
    "        about_selectors = [\n",
    "            \"div.display-flex.ph5.pv-top-card\",\n",
    "            \"section[data-section='summary']\",\n",
    "            \"div.pv-about-section\",\n",
    "            \"section.artdeco-card.pv-profile-card\",\n",
    "            \"div.ph5.pb5\"\n",
    "        ]\n",
    "        \n",
    "        about_text_selectors = [\n",
    "            \"div.inline-show-more-text--is-collapsed span[aria-hidden='true']\",\n",
    "            \"div.pv-about__summary-text span[aria-hidden='true']\",\n",
    "            \"div.inline-show-more-text span[aria-hidden='true']\",\n",
    "            \"span.break-words\",\n",
    "            \"div.display-flex span[aria-hidden='true']\"\n",
    "        ]\n",
    "        \n",
    "        about_div = None\n",
    "        for selector in about_selectors:\n",
    "            try:\n",
    "                about_div = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                )\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if about_div:\n",
    "            about_element = find_element_with_fallbacks(about_div, about_text_selectors)\n",
    "            if about_element:\n",
    "                data[\"about\"] = about_element.text.strip()\n",
    "            else:\n",
    "                data[\"about\"] = \"\"\n",
    "        else:\n",
    "            data[\"about\"] = \"\"\n",
    "        print(\"  ✅ About section scraped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not scrape About section: {str(e)}\")\n",
    "        data[\"about\"] = \"\"\n",
    "\n",
    "    # — Followers —\n",
    "    try:\n",
    "        print(\"  → Scraping Followers...\")\n",
    "        follower_selectors = [\n",
    "            \"p.pvs-header__optional-link span[aria-hidden='true']\",\n",
    "            \"span.t-14.t-normal.t-black--light\",\n",
    "            \"span.pv-top-card--list-bullet\",\n",
    "            \"li.pv-top-card--list-bullet span\"\n",
    "        ]\n",
    "        \n",
    "        fol_element = find_element_with_fallbacks(driver, follower_selectors)\n",
    "        data[\"followers\"] = fol_element.text.strip() if fol_element else \"\"\n",
    "        print(\"  ✅ Followers scraped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not scrape Followers: {str(e)}\")\n",
    "        data[\"followers\"] = \"\"\n",
    "\n",
    "    # — Experience —\n",
    "    exps = []\n",
    "    try:\n",
    "        print(\"  → Scraping Experience section...\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.getElementById('experience').offsetTop);\")\n",
    "        time.sleep(1)\n",
    "        exp_section = driver.find_element(By.ID, \"experience\")\n",
    "        ul = exp_section.find_element(By.XPATH, \"./following-sibling::div//ul\")\n",
    "        items = ul.find_elements(By.TAG_NAME, \"li\")\n",
    "        print(f\"  → Found {len(items)} experience entries\")\n",
    "        \n",
    "        current_exp = None  # Track current experience for sub-items\n",
    "        \n",
    "        for i, li in enumerate(items, 1):\n",
    "            try:\n",
    "                # Check if this li has the main experience container (main job entry)\n",
    "                exp_containers = li.find_elements(By.CSS_SELECTOR, \"div[data-view-name='profile-component-entity']\")\n",
    "                \n",
    "                if exp_containers:\n",
    "                    # This is a main experience entry\n",
    "                    exp_container = exp_containers[0]\n",
    "                    \n",
    "                    # Job Title - Look for the specific pattern we identified\n",
    "                    title_selectors = [\n",
    "                        \"div.mr1.hoverable-link-text.t-bold span[aria-hidden='true']\",\n",
    "                        \"div.display-flex.align-items-center.mr1.hoverable-link-text.t-bold span[aria-hidden='true']\",\n",
    "                        \"div.hoverable-link-text.t-bold span[aria-hidden='true']\",\n",
    "                        \"span.mr1.t-bold span[aria-hidden='true']\"\n",
    "                    ]\n",
    "                    \n",
    "                    # Company Name - First span.t-14.t-normal\n",
    "                    company_selectors = [\n",
    "                        \"span.t-14.t-normal span[aria-hidden='true']\",\n",
    "                        \"span.t-14.t-normal\",\n",
    "                        \"a[data-field='experience_company_logo'] span.t-14.t-normal span[aria-hidden='true']\"\n",
    "                    ]\n",
    "                    \n",
    "                    # Extract basic job info\n",
    "                    title_element = find_element_with_fallbacks(exp_container, title_selectors)\n",
    "                    company_element = find_element_with_fallbacks(exp_container, company_selectors)\n",
    "                    \n",
    "                    title = title_element.text.strip() if title_element else \"\"\n",
    "                    company = company_element.text.strip() if company_element else \"\"\n",
    "                    \n",
    "                    # Clean up company name (remove \"· Full-time\" etc.)\n",
    "                    if company and '·' in company:\n",
    "                        company = company.split('·')[0].strip()\n",
    "                    \n",
    "                    if title or company:  # Only create if we got at least title or company\n",
    "                        current_exp = {\n",
    "                            \"exp_title\": title,\n",
    "                            \"exp_company\": company\n",
    "                        }\n",
    "                        exps.append(current_exp)  # Add immediately, no need to track sub-items\n",
    "                        print(f\"  → Scraped experience {i}/{len(items)}: {title} at {company}\")\n",
    "                    else:\n",
    "                        print(f\"  ⚠️ Could not extract main data for experience {i}\")\n",
    "                        \n",
    "                else:\n",
    "                    # Skip sub-items since we're not tracking dates/location anymore\n",
    "                    print(f\"  ⏩ Skipping sub-item {i}/{len(items)}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Could not process item {i}: {str(e)}\")\n",
    "                continue\n",
    "        print(\"  ✅ Experience section scraped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not access Experience section: {str(e)}\")\n",
    "    data[\"experience\"] = exps\n",
    "\n",
    "    # — Skills —\n",
    "    skills = []\n",
    "    try:\n",
    "        print(\"  → Scraping Skills section...\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.getElementById('skills').offsetTop);\")\n",
    "        time.sleep(1)\n",
    "        sk_section = driver.find_element(By.ID, \"skills\")\n",
    "        sk_ul = sk_section.find_element(By.XPATH, \"./following-sibling::div//ul\")\n",
    "        sk_items = sk_ul.find_elements(By.TAG_NAME, \"li\")\n",
    "        print(f\"  → Found {len(sk_items)} skills\")\n",
    "        \n",
    "        skill_count = 0\n",
    "        for i, sk in enumerate(sk_items, 1):\n",
    "            try:\n",
    "                # Look for skill name (should not contain numbers like \"8 endorsements\")\n",
    "                skill_name_selectors = [\n",
    "                    \"span[aria-hidden='true']\",\n",
    "                    \"div.hoverable-link-text span[aria-hidden='true']\",\n",
    "                    \"a span[aria-hidden='true']\",\n",
    "                    \".pvs-entity__path-node span\"\n",
    "                ]\n",
    "                \n",
    "                name_element = find_element_with_fallbacks(sk, skill_name_selectors)\n",
    "                name = name_element.text.strip() if name_element else \"\"\n",
    "                \n",
    "                # Only add if we got a skill name AND it doesn't look like an endorsement count\n",
    "                if name and not any(indicator in name.lower() for indicator in ['endorsement', 'endorsing', 'connection']):\n",
    "                    # Additional check: skip if it's just numbers or contains only numbers and common words\n",
    "                    if not (name.replace(' ', '').isdigit() or \n",
    "                           any(name.lower().startswith(num) for num in ['1 ', '2 ', '3 ', '4 ', '5 ', '6 ', '7 ', '8 ', '9 '])):\n",
    "                        skills.append(name)  # Just store the skill name as a string\n",
    "                        skill_count += 1\n",
    "                        print(f\"  → Scraped skill {skill_count}: {name}\")\n",
    "                    else:\n",
    "                        print(f\"  ⏩ Skipping endorsement count: {name}\")\n",
    "                else:\n",
    "                    if name:\n",
    "                        print(f\"  ⏩ Skipping endorsement item: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Could not scrape skill {i}: {str(e)}\")\n",
    "                continue\n",
    "        print(\"  ✅ Skills section scraped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not access Skills section: {str(e)}\")\n",
    "    data[\"skills\"] = skills\n",
    "\n",
    "    # — Education —\n",
    "    education = []\n",
    "    try:\n",
    "        print(\"  → Scraping Education section...\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.getElementById('education').offsetTop);\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        education_section = driver.find_element(By.ID, \"education\")\n",
    "        ul = education_section.find_element(By.XPATH, \"./following-sibling::div//ul\")\n",
    "        items = ul.find_elements(By.TAG_NAME, \"li\")\n",
    "        print(f\"  → Found {len(items)} education entries\")\n",
    "        \n",
    "        for i, li in enumerate(items, 1):\n",
    "            try:\n",
    "                # Find education containers (similar pattern to experience)\n",
    "                edu_containers = li.find_elements(By.CSS_SELECTOR, \"div[data-view-name='profile-component-entity']\")\n",
    "                \n",
    "                if edu_containers:\n",
    "                    edu_container = edu_containers[0]\n",
    "                    \n",
    "                    # School Name - Look for the bold hoverable link text\n",
    "                    school_selectors = [\n",
    "                        \"div.mr1.hoverable-link-text.t-bold span[aria-hidden='true']\",\n",
    "                        \"div.hoverable-link-text.t-bold span[aria-hidden='true']\",\n",
    "                        \"div.display-flex.align-items-center.mr1.hoverable-link-text.t-bold span[aria-hidden='true']\"\n",
    "                    ]\n",
    "                    \n",
    "                    # Degree - Look for span.t-14.t-normal (but not the one with t-black--light)\n",
    "                    degree_selectors = [\n",
    "                        \"span.t-14.t-normal span[aria-hidden='true']\",\n",
    "                        \"span.t-14.t-normal\"\n",
    "                    ]\n",
    "                    \n",
    "                    # Years - Look for the caption wrapper\n",
    "                    years_selectors = [\n",
    "                        \"span.pvs-entity__caption-wrapper[aria-hidden='true']\",\n",
    "                        \"span.t-14.t-normal.t-black--light span.pvs-entity__caption-wrapper[aria-hidden='true']\",\n",
    "                        \"span.t-14.t-normal.t-black--light span[aria-hidden='true']\"\n",
    "                    ]\n",
    "                    \n",
    "                    # Extract data\n",
    "                    school_element = find_element_with_fallbacks(edu_container, school_selectors)\n",
    "                    degree_element = find_element_with_fallbacks(edu_container, degree_selectors)\n",
    "                    years_element = find_element_with_fallbacks(edu_container, years_selectors)\n",
    "                    \n",
    "                    school = school_element.text.strip() if school_element else \"\"\n",
    "                    degree = degree_element.text.strip() if degree_element else \"\"\n",
    "                    years = years_element.text.strip() if years_element else \"\"\n",
    "                    \n",
    "                    if school:  # Only add if we got at least a school name\n",
    "                        education.append({\n",
    "                            \"school\": school,\n",
    "                            \"degree\": degree,\n",
    "                            \"years\": years\n",
    "                        })\n",
    "                        print(f\"  → Scraped education {i}/{len(items)}: {school} - {degree} ({years})\")\n",
    "                    else:\n",
    "                        print(f\"  ⚠️ Could not extract school name for education {i}\")\n",
    "                else:\n",
    "                    # Skip sub-items for education (like activities)\n",
    "                    print(f\"  ⏩ Skipping education sub-item {i}/{len(items)}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Could not process education item {i}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        print(\"  ✅ Education section scraped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Could not access Education section: {str(e)}\")\n",
    "    \n",
    "    data[\"education\"] = education\n",
    "    print(\"✅ Profile scraping completed\")\n",
    "    return data\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load existing progress from pickle file\"\"\"\n",
    "    if os.path.exists(PROGRESS_PICKLE):\n",
    "        try:\n",
    "            with open(PROGRESS_PICKLE, 'rb') as f:\n",
    "                progress = pickle.load(f)\n",
    "                print(f\"📂 Loaded progress: {len(progress['results'])} profiles already scraped\")\n",
    "                return progress['results'], progress['processed_indices']\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load progress file: {e}\")\n",
    "    return [], set()\n",
    "\n",
    "def save_progress(results, processed_indices):\n",
    "    \"\"\"Save current progress to pickle file\"\"\"\n",
    "    try:\n",
    "        progress = {\n",
    "            'results': results,\n",
    "            'processed_indices': processed_indices\n",
    "        }\n",
    "        with open(PROGRESS_PICKLE, 'wb') as f:\n",
    "            pickle.dump(progress, f)\n",
    "        print(f\"💾 Progress saved: {len(results)} profiles completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not save progress: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"\\n🚀 Starting LinkedIn profile scraper...\")\n",
    "    \n",
    "    # Load existing progress\n",
    "    results, processed_indices = load_progress()\n",
    "    \n",
    "    driver = init_driver()\n",
    "    \n",
    "    # Log in first before scraping\n",
    "    if not linkedin_login(driver):\n",
    "        driver.quit()\n",
    "        raise Exception(\"Failed to log into LinkedIn. Please check credentials.\")\n",
    "    \n",
    "    print(f\"\\n📖 Reading input CSV: {INPUT_CSV}\")\n",
    "    df_urls = pd.read_csv(INPUT_CSV)\n",
    "    print(f\"✅ Found {len(df_urls)} total profiles\")\n",
    "    \n",
    "    # Filter out headless URLs and already processed ones\n",
    "    valid_profiles = []\n",
    "    for idx, row in df_urls.iterrows():\n",
    "        url = row[\"profile_url\"]\n",
    "        if \"https://www.linkedin.com/search/results/people/headless\" in url:\n",
    "            # print(f\"⏩ Skipping headless URL: {url}\")\n",
    "            continue\n",
    "        if idx in processed_indices:\n",
    "            # print(f\"⏩ Already processed profile {idx+1}\")\n",
    "            continue\n",
    "        valid_profiles.append((idx, row))\n",
    "    \n",
    "    total_remaining = len(valid_profiles)\n",
    "    print(f\"📊 {len(processed_indices)} already completed, {total_remaining} remaining to process\")\n",
    "\n",
    "    for count, (idx, row) in enumerate(valid_profiles, 1):\n",
    "        url = row[\"profile_url\"]\n",
    "        print(f\"\\n🔄 Processing profile {count}/{total_remaining} (Row {idx+1}): {url}\")\n",
    "        \n",
    "        try:\n",
    "            profile_data = scrape_profile(driver, url)\n",
    "            results.append({**row.to_dict(), **profile_data})\n",
    "            processed_indices.add(idx)\n",
    "            \n",
    "            # Save progress every 5 profiles\n",
    "            if count % 5 == 0:\n",
    "                save_progress(results, processed_indices)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error on {url}: {e}\")\n",
    "            processed_indices.add(idx)  # Mark as processed even if failed\n",
    "            continue\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Final save\n",
    "    save_progress(results, processed_indices)\n",
    "\n",
    "    # Normalize lists for CSV (JSON-dump experience, skills, education)\n",
    "    print(\"\\n📊 Preparing final results...\")\n",
    "    for r in results:\n",
    "        r[\"experience\"] = json.dumps(r[\"experience\"])  # List of {exp_title, exp_company}\n",
    "        r[\"skills\"]     = json.dumps(r[\"skills\"])      # List of skill name strings\n",
    "        r[\"education\"]  = json.dumps(r[\"education\"])   # List of {school, degree, years}\n",
    "\n",
    "    df_prof = pd.DataFrame(results)\n",
    "    df_prof.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"✅ Done! Saved {len(results)} profiles to {OUTPUT_CSV}\")\n",
    "    \n",
    "    # Clean up progress file\n",
    "    if os.path.exists(PROGRESS_PICKLE):\n",
    "        os.remove(PROGRESS_PICKLE)\n",
    "        print(\"🧹 Cleaned up temporary progress file\")\n",
    "    \n",
    "    print(f\"\\n📈 Final Summary:\")\n",
    "    print(f\"   • Total profiles in CSV: {len(df_urls)}\")\n",
    "    print(f\"   • Successfully scraped: {len(results)}\")\n",
    "    print(f\"   • Headless URLs skipped: {len([r for _, r in df_urls.iterrows() if 'https://www.linkedin.com/search/results/people/headless' in r['profile_url']])}\")\n",
    "    print(f\"\\nSample of scraped profiles:\")\n",
    "    print(df_prof.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prof = pd.read_csv('mschf_profiles_detailed_full.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
